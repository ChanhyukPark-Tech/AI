{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7917bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25dbdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "657632b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "train_data = datasets.MNIST('D:₩datasets', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.MNIST('D:₩datasets', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59b7ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = 28*28\n",
    "        self.out_dim = 10\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.in_dim,512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.fc3 = nn.Linear(256,128)\n",
    "        self.fc4 = nn.Linear(128,64)\n",
    "        self.fc5 = nn.Linear(64,self.out_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        a1 = self.relu(self.fc1(x.view(-1,self.in_dim)))\n",
    "        a2 = self.relu(self.fc2(a1))\n",
    "        a3 = self.relu(self.fc3(a2))\n",
    "        a4 = self.relu(self.fc4(a3))\n",
    "        logit = self.fc5(a4)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1333a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ae697c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2000 2.21027131074667\n",
      "1 4000 0.8288113939836621\n",
      "2 2000 0.3297104379730299\n",
      "2 4000 0.24130152406706476\n",
      "3 2000 0.17468786945252213\n",
      "3 4000 0.14869224634795683\n",
      "4 2000 0.1171955313545186\n",
      "4 4000 0.10754716007318348\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader,\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      4\u001b[0m     inputs,labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m----> 5\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m      8\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs,labels)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/optim/optimizer.py:213\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m    215\u001b[0m                 p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py:1092\u001b[0m, in \u001b[0;36mTensor.grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Tensor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrename(names)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;124;03m    This attribute is ``None`` by default and becomes a Tensor the first time a call to\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    :func:`backward` computes gradients for ``self``.\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m    The attribute will then contain the gradients computed and future calls to\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m    :func:`backward` will accumulate (add) gradients into it.\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;66;03m# TODO mypy doesn't support @property, see: https://github.com/python/mypy/issues/6185\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader,0):\n",
    "        inputs,labels = data\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print(epoch + 1,i+1,running_loss/2000)\n",
    "            \n",
    "            running_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a55990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac4144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef1e018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
